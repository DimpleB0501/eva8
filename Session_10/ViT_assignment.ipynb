{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNJ9DVcJsgHzUPbsM5JXFVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DimpleB0501/eva8/blob/main/Session_10/ViT_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pcdlAJM51sd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR='./data'"
      ],
      "metadata": {
        "id": "mKq2Y68Y65jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"device:\", DEVICE)"
      ],
      "metadata": {
        "id": "JNMScexs67Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT"
      ],
      "metadata": {
        "id": "iG2WdHKrUPzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patch_size = 8 # P"
      ],
      "metadata": {
        "id": "KnZNypFBN3-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class which subclasses nn.Module\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
        "    \n",
        "    Args:\n",
        "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
        "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
        "    \"\"\" \n",
        "    # 2. Initialize the class with appropriate variables\n",
        "    def __init__(self, \n",
        "                 in_channels:int=3,\n",
        "                 patch_size:int=8,\n",
        "                 embedding_dim:int=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 3. Create a layer to turn an image into patches\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
        "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
        "                                  end_dim=3)\n",
        "\n",
        "    # 5. Define the forward method \n",
        "    def forward(self, x):\n",
        "        # Create assertion to check that inputs are the correct shape\n",
        "        image_resolution = x.shape[-1]\n",
        "        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
        "        \n",
        "        # Perform the forward pass\n",
        "        x_patched = self.patcher(x)\n",
        "        x_flattened = self.flatten(x_patched) \n",
        "        # 6. Make sure the output shape has the right order \n",
        "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
      ],
      "metadata": {
        "id": "RBU15ditNE8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MultiheadSelfAttentionBlock(nn.Module):\n",
        "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=256, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=8, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
        "        super().__init__()\n",
        "        \n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "        \n",
        "        # 4. Create the Multi-Head Attention (MSA) layer\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                    num_heads=num_heads,\n",
        "                                                    dropout=attn_dropout,\n",
        "                                                    batch_first=True) # does our batch dimension come first?\n",
        "        \n",
        "    # 5. Create a forward() method to pass the data throguh the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        attn_output, _ = self.multihead_attn(query=x, # query embeddings \n",
        "                                             key=x, # key embeddings\n",
        "                                             value=x, # value embeddings\n",
        "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
        "        return attn_output"
      ],
      "metadata": {
        "id": "2NvwVm-YMOEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=256, # Hidden Size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=512, # MLP size from Table 1 for ViT-Base\n",
        "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
        "        super().__init__()\n",
        "        \n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "        \n",
        "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=mlp_size),\n",
        "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
        "                      out_features=embedding_dim), # take back to embedding_dim\n",
        "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
        "        )\n",
        "    \n",
        "    # 5. Create a forward() method to pass the data throguh the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "phWmsluxMFOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=256, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=8, # Heads from Table 1 for ViT-Base\n",
        "                 mlp_size:int=512, # MLP size from Table 1 for ViT-Base\n",
        "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create MSA block (equation 2)\n",
        "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                     num_heads=num_heads,\n",
        "                                                     attn_dropout=attn_dropout)\n",
        "        \n",
        "        # 4. Create MLP block (equation 3)\n",
        "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
        "                                   mlp_size=mlp_size,\n",
        "                                   dropout=mlp_dropout)\n",
        "        \n",
        "    # 5. Create a forward() method  \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # 6. Create residual connection for MSA block (add the input to the output)\n",
        "        x =  self.msa_block(x) + x \n",
        "        \n",
        "        # 7. Create residual connection for MLP block (add the input to the output)\n",
        "        x = self.mlp_block(x) + x \n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "Dm333fbTL67J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a ViT class that inherits from nn.Module\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 img_size:int=32, # Training resolution from Table 3 in ViT paper\n",
        "                 in_channels:int=3, # Number of channels in input image\n",
        "                 patch_size:int=8, # Patch size\n",
        "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
        "                 embedding_dim:int=256, # Hidden size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=512, # MLP size from Table 1 for ViT-Base\n",
        "                 num_heads:int=8, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0, # Dropout for attention projection\n",
        "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers \n",
        "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
        "                 num_classes:int=10): # Default for ImageNet but can customize this\n",
        "        super().__init__() # don't forget the super().__init__()!\n",
        "        \n",
        "        # 3. Make the image size is divisble by the patch size \n",
        "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
        "        \n",
        "        # 4. Calculate number of patches (height * width/patch^2)\n",
        "        self.num_patches = (img_size * img_size) // patch_size**2\n",
        "                 \n",
        "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
        "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
        "                                            requires_grad=True)\n",
        "        \n",
        "        # 6. Create learnable position embedding\n",
        "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
        "                                               requires_grad=True)\n",
        "                \n",
        "        # 7. Create embedding dropout value\n",
        "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "        \n",
        "        # 8. Create patch embedding layer\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                              patch_size=patch_size,\n",
        "                                              embedding_dim=embedding_dim)\n",
        "        \n",
        "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential()) \n",
        "        # Note: The \"*\" means \"all\"\n",
        "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
        "                                                                            num_heads=num_heads,\n",
        "                                                                            mlp_size=mlp_size,\n",
        "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
        "       \n",
        "        # 10. Create classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "            nn.Linear(in_features=embedding_dim, \n",
        "                      out_features=num_classes)\n",
        "        )\n",
        "    \n",
        "    # 11. Create a forward() method\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # 12. Get batch size\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
        "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
        "\n",
        "        # 14. Create patch embedding (equation 1)\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # 15. Concat class embedding and patch embedding (equation 1)\n",
        "        x = torch.cat((class_token, x), dim=1)\n",
        "\n",
        "        # 16. Add position embedding to patch embedding (equation 1) \n",
        "        x = self.position_embedding + x\n",
        "\n",
        "        # 17. Run embedding dropout (Appendix B.1)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # 19. Put 0 index logit through classifier (equation 4)\n",
        "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
        "\n",
        "        return x       "
      ],
      "metadata": {
        "id": "W8NWUUBBL5A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES, IMAGE_SIZE = 10, 32\n",
        "model = ViT(num_classes=10)"
      ],
      "metadata": {
        "id": "DCbQKfVn7crL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)"
      ],
      "metadata": {
        "id": "kLBniG2e7ejz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))"
      ],
      "metadata": {
        "id": "ITG0pdF77hbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model summary"
      ],
      "metadata": {
        "id": "0uKxMPfRE-3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchinfo"
      ],
      "metadata": {
        "id": "Swo-tfU1PSXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "YV_SyfAsPAHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 32\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 25 #25\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-1\n",
        "\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32, scale=(0.75, 1.0), ratio=(1.0, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandAugment(num_ops=1, magnitude=8),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "id": "TfmSvhmw7jp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "\n",
        "clip_norm = True\n",
        "lr_schedule = lambda t: np.interp([t], [0, EPOCHS*2//5, EPOCHS*4//5, EPOCHS], \n",
        "                                  [0, 0.01, 0.01/20.0, 0])[0]\n",
        "\n",
        "model = nn.DataParallel(model, device_ids=[0]).cuda()\n",
        "opt = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss, train_acc, n = 0, 0, 0\n",
        "    for i, (X, y) in enumerate(trainloader):\n",
        "        model.train()\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "\n",
        "        lr = lr_schedule(epoch + (i + 1)/len(trainloader))\n",
        "        opt.param_groups[0].update(lr=lr)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip_norm:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        train_acc += (output.max(1)[1] == y).sum().item()\n",
        "        n += y.size(0)\n",
        "        \n",
        "    model.eval()\n",
        "    test_acc, m = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(testloader):\n",
        "            X, y = X.cuda(), y.cuda()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(X)\n",
        "            test_acc += (output.max(1)[1] == y).sum().item()\n",
        "            m += y.size(0)\n",
        "\n",
        "    print(f'ConvMixer: Epoch: {epoch} | Train Acc: {train_acc/n:.4f}, Test Acc: {test_acc/m:.4f}, Time: {time.time() - start:.1f}, lr: {lr:.6f}')\n"
      ],
      "metadata": {
        "id": "vNqeKdnO7m30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gradcam on 10 Misclassified images"
      ],
      "metadata": {
        "id": "ZZPY46By_fXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install grad-cam"
      ],
      "metadata": {
        "id": "g_EgVJUA_2cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([n for n, _ in model.module.classifier.named_children()])"
      ],
      "metadata": {
        "id": "3tIv5LuT5dmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "cY8W0VVjGGCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_layers_name = [name for name, module in model.named_modules() if isinstance(module, nn.GELU)]"
      ],
      "metadata": {
        "id": "iYrpI3niF7qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (target_layers_name)"
      ],
      "metadata": {
        "id": "1QWkfr_UGxjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2471, 0.2435, 0.2616)"
      ],
      "metadata": {
        "id": "uLulzUBjV5-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG9PMnJSDg2a"
      },
      "outputs": [],
      "source": [
        "def wrong_predictions(model, test_loader, device):\n",
        "    wrong_images=[]\n",
        "    wrong_label=[]\n",
        "    correct_label=[]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)        \n",
        "            pred = output.argmax(dim=1, keepdim=True).squeeze()  # get the index of the max log-probability\n",
        "\n",
        "            wrong_pred = (pred.eq(target.view_as(pred)) == False)\n",
        "            wrong_images.append(data[wrong_pred])\n",
        "            wrong_label.append(pred[wrong_pred])\n",
        "            correct_label.append(target.view_as(pred)[wrong_pred])\n",
        "            wrong_predictions = list(zip(torch.cat(wrong_images),torch.cat(wrong_label),torch.cat(correct_label)))\n",
        "        print(f'Total wrong predictions are {len(wrong_predictions)}')\n",
        "\n",
        "    return wrong_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oScQfnb4CwZ4"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "import cv2\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GradCAM:\n",
        "    \"\"\" Class for extracting activations and \n",
        "    registering gradients from targetted intermediate layers \n",
        "    target_layers = list of convolution layer index as shown in summary\n",
        "    \"\"\"\n",
        "    def __init__(self, model, candidate_layers=None):\n",
        "        def save_fmaps(key):\n",
        "            def forward_hook(module, input, output):\n",
        "                self.fmap_pool[key] = output.detach()\n",
        "\n",
        "            return forward_hook\n",
        "\n",
        "        def save_grads(key):\n",
        "            def backward_hook(module, grad_in, grad_out):\n",
        "                self.grad_pool[key] = grad_out[0].detach()\n",
        "\n",
        "            return backward_hook\n",
        "\n",
        "        self.device = next(model.parameters()).device\n",
        "        self.model = model\n",
        "        self.handlers = []  # a set of hook function handlers\n",
        "        self.fmap_pool = {}\n",
        "        self.grad_pool = {}\n",
        "        self.candidate_layers = candidate_layers  # list\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if self.candidate_layers is None or name in self.candidate_layers:\n",
        "                self.handlers.append(module.register_forward_hook(save_fmaps(name)))\n",
        "                self.handlers.append(module.register_backward_hook(save_grads(name)))\n",
        "\n",
        "    def _encode_one_hot(self, ids):\n",
        "        one_hot = torch.zeros_like(self.nll).to(self.device)\n",
        "        #print(one_hot.shape)\n",
        "        one_hot.scatter_(1, ids, 1.0)\n",
        "        return one_hot\n",
        "\n",
        "    def forward(self, image):\n",
        "        self.image_shape = image.shape[2:] # HxW\n",
        "        self.nll = self.model(image)\n",
        "        #self.probs = F.softmax(self.logits, dim=1)\n",
        "        return self.nll.sort(dim=1, descending=True)  # ordered results\n",
        "\n",
        "    def backward(self, ids):\n",
        "        \"\"\"\n",
        "        Class-specific backpropagation\n",
        "        \"\"\"\n",
        "        one_hot = self._encode_one_hot(ids)\n",
        "        self.model.zero_grad()\n",
        "        self.nll.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "    def remove_hook(self):\n",
        "        \"\"\"\n",
        "        Remove all the forward/backward hook functions\n",
        "        \"\"\"\n",
        "        for handle in self.handlers:\n",
        "            handle.remove()\n",
        "\n",
        "    def _find(self, pool, target_layer):\n",
        "        if target_layer in pool.keys():\n",
        "            return pool[target_layer]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid layer name: {}\".format(target_layer))\n",
        "\n",
        "    def generate(self, target_layer):\n",
        "        fmaps = self._find(self.fmap_pool, target_layer)\n",
        "        grads = self._find(self.grad_pool, target_layer)\n",
        "        weights = F.adaptive_avg_pool2d(grads, 1)\n",
        "        gcam = torch.mul(fmaps, weights).sum(dim=1, keepdim=True)\n",
        "        gcam = F.relu(gcam)\n",
        "        # need to capture image size during forward pass\n",
        "        gcam = gcam[:, :, :].reshape(gcam.size(0),gcam.size(1), 16, 16)\n",
        "        # Bring the channels to the first dimension,\n",
        "        # like in CNNs.\n",
        "        #print (self.image_shape, gcam.shape)\n",
        "        gcam = F.interpolate(\n",
        "            gcam, self.image_shape, mode=\"bilinear\", align_corners=True\n",
        "        )\n",
        "\n",
        "        # scale output between 0,1\n",
        "        B, C, H, W = gcam.shape\n",
        "        gcam = gcam.view(B, -1)\n",
        "        gcam -= gcam.min(dim=1, keepdim=True)[0]\n",
        "        gcam /= gcam.max(dim=1, keepdim=True)[0]\n",
        "        gcam = gcam.view(B, C, H, W)\n",
        "\n",
        "        return gcam\n",
        "\n",
        "def generate_gradcam(misclassified_images, model, target_layers, device):\n",
        "    images=[]\n",
        "    labels=[]\n",
        "    for i, (img, pred, correct) in enumerate(misclassified_images):\n",
        "        images.append(img)\n",
        "        labels.append(correct)\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # map input to device\n",
        "    images = torch.stack(images).to(device)\n",
        " \n",
        "    \n",
        "    # set up grad cam\n",
        "    gcam = GradCAM(model, target_layers)\n",
        "    \n",
        "    # forward pass\n",
        "    probs, ids = gcam.forward(images)\n",
        "\n",
        "    # outputs agaist which to compute gradients\n",
        "    ids_ = torch.LongTensor(labels).view(len(images),-1).to(device)\n",
        "    \n",
        "    # backward pass\n",
        "    gcam.backward(ids=ids_)\n",
        "    layers = []\n",
        "    for i in range(len(target_layers)):\n",
        "        target_layer = target_layers[i]\n",
        "        print(\"Generating Grad-CAM @{}\".format(target_layer))\n",
        "        # Grad-CAM\n",
        "        layers.append(gcam.generate(target_layer=target_layer))\n",
        "        #print (\"here4\")\n",
        "        \n",
        "    # remove hooks when done\n",
        "    gcam.remove_hook()\n",
        "    return layers, probs, ids\n",
        "\n",
        "def plot_gradcam(gcam_layers, target_layers, class_names, image_size, predicted, misclassified_images, mean, std):\n",
        "    \n",
        "    images=[]\n",
        "    labels=[]\n",
        "    for i, (img, pred, correct) in enumerate(misclassified_images):\n",
        "        images.append(img)\n",
        "        labels.append(correct)\n",
        "\n",
        "    c = len(images)+1\n",
        "    r = len(target_layers)+2\n",
        "    fig = plt.figure(figsize=(30,14))\n",
        "    fig.subplots_adjust(hspace=0.01, wspace=0.01)\n",
        "    ax = plt.subplot(r, c, 1)\n",
        "    ax.text(0.3,-0.5, \"INPUT\", fontsize=14)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    for i in range(len(target_layers)):\n",
        "        target_layer = target_layers[i]\n",
        "        ax = plt.subplot(r, c, c*(i+1)+1)\n",
        "        ax.text(0.3,-0.5, target_layer, fontsize=14)\n",
        "        plt.axis('off')\n",
        "\n",
        "        for j in range(len(images)):\n",
        "            #print(\"processing image \" + str(j+1))\n",
        "            img = np.uint8(255*unnormalize(images[j].view(image_size), mean, std))\n",
        "            if i==0:\n",
        "                ax = plt.subplot(r, c, j+2)\n",
        "                ax.text(0, 0.2, f\"actual: {class_names[labels[j]]} \\npredicted: {class_names[predicted[j][0]]}\", fontsize=12)\n",
        "                plt.axis('off')\n",
        "                plt.subplot(r, c, c+j+2)\n",
        "                plt.imshow(img)\n",
        "                plt.axis('off')\n",
        "                \n",
        "                heatmap = 1-gcam_layers[i][j].cpu().numpy()[0] # reverse the color map\n",
        "                heatmap = np.uint8(255 * heatmap)\n",
        "                heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "                superimposed_img = cv2.resize(cv2.addWeighted(img, 0.5, heatmap, 0.5, 0), (128,128))\n",
        "                plt.subplot(r, c, (i+2)*c+j+2)\n",
        "                plt.imshow(superimposed_img, interpolation='bilinear')\n",
        "                plt.axis('off')\n",
        "        \n",
        "    plt.show()\n",
        "    \n",
        "def unnormalize(img, mean, std):\n",
        "    img = img.cpu().numpy().astype(dtype=np.float32)\n",
        "  \n",
        "    for i in range(img.shape[0]):\n",
        "        img[i] = (img[i]*std[i])+mean[i]\n",
        "  \n",
        "    return np.transpose(img, (1,2,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHU7PkvfC45R"
      },
      "outputs": [],
      "source": [
        "target_layers = ['module.transformer_encoder.11']\n",
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "misclassified_images  = wrong_predictions(model, testloader, DEVICE)\n",
        "gradcam_output, probs, predicted_classes = generate_gradcam(misclassified_images[:10], model, target_layers, DEVICE)\n",
        "plot_gradcam(gradcam_output, target_layers, classes, (3, 32, 32),predicted_classes, misclassified_images[:10], mean, std)"
      ]
    }
  ]
}