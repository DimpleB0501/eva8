{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNGnFOZl3Y0YBgnmtCL+6Yq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DimpleB0501/eva8/blob/main/Session_10/backup/ViT_backup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pcdlAJM51sd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR='./data'"
      ],
      "metadata": {
        "id": "mKq2Y68Y65jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"device:\", DEVICE)"
      ],
      "metadata": {
        "id": "JNMScexs67Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, *layers):\n",
        "        super().__init__()\n",
        "        self.residual = nn.Sequential(*layers)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.gamma * self.residual(x)"
      ],
      "metadata": {
        "id": "-fof5Eqw6963"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormChannels(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, -1)\n",
        "        x = self.norm(x)\n",
        "        x = x.transpose(-1, 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NsnboW3L6-kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, head_channels, shape):\n",
        "        super().__init__()\n",
        "        self.heads = out_channels // head_channels\n",
        "        self.head_channels = head_channels\n",
        "        self.scale = head_channels**-0.5\n",
        "        \n",
        "        self.to_keys = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.to_queries = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.to_values = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.unifyheads = nn.Conv2d(out_channels, out_channels, 1)\n",
        "        \n",
        "        height, width = shape\n",
        "        self.pos_enc = nn.Parameter(torch.Tensor(self.heads, (2 * height - 1) * (2 * width - 1)))\n",
        "        self.register_buffer(\"relative_indices\", self.get_indices(height, width))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, _, h, w = x.shape\n",
        "        \n",
        "        keys = self.to_keys(x).view(b, self.heads, self.head_channels, -1)\n",
        "        values = self.to_values(x).view(b, self.heads, self.head_channels, -1)\n",
        "        queries = self.to_queries(x).view(b, self.heads, self.head_channels, -1)\n",
        "        \n",
        "        att = keys.transpose(-2, -1) @ queries\n",
        "        \n",
        "        indices = self.relative_indices.expand(self.heads, -1)\n",
        "        rel_pos_enc = self.pos_enc.gather(-1, indices)\n",
        "        rel_pos_enc = rel_pos_enc.unflatten(-1, (h * w, h * w))\n",
        "        \n",
        "        att = att * self.scale + rel_pos_enc\n",
        "        att = F.softmax(att, dim=-2)\n",
        "        \n",
        "        out = values @ att\n",
        "        out = out.view(b, -1, h, w)\n",
        "        out = self.unifyheads(out)\n",
        "        return out\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_indices(h, w):\n",
        "        y = torch.arange(h, dtype=torch.long)\n",
        "        x = torch.arange(w, dtype=torch.long)\n",
        "        \n",
        "        y1, x1, y2, x2 = torch.meshgrid(y, x, y, x, indexing='ij')\n",
        "        indices = (y1 - y2 + h - 1) * (2 * w - 1) + x1 - x2 + w - 1\n",
        "        indices = indices.flatten()\n",
        "        \n",
        "        return indices"
      ],
      "metadata": {
        "id": "nSlqYAEl7Alz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, mult=4):\n",
        "        hidden_channels = in_channels * mult\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(hidden_channels, out_channels, 1)   \n",
        "        )"
      ],
      "metadata": {
        "id": "EN-2_Lg67MZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Sequential):\n",
        "    def __init__(self, channels, head_channels, shape, p_drop=0.):\n",
        "        super().__init__(\n",
        "            Residual(\n",
        "                LayerNormChannels(channels),\n",
        "                SelfAttention2d(channels, channels, head_channels, shape),\n",
        "                nn.Dropout(p_drop)\n",
        "            ),\n",
        "            Residual(\n",
        "                LayerNormChannels(channels),\n",
        "                FeedForward(channels, channels),\n",
        "                nn.Dropout(p_drop)\n",
        "            )\n",
        "        )"
      ],
      "metadata": {
        "id": "BqL8uyd77NE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerStack(nn.Sequential):\n",
        "    def __init__(self, num_blocks, channels, head_channels, shape, p_drop=0.):\n",
        "        layers = [TransformerBlock(channels, head_channels, shape, p_drop) for _ in range(num_blocks)]\n",
        "        super().__init__(*layers)"
      ],
      "metadata": {
        "id": "FmyvkI2P7Osd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToPatches(nn.Sequential):\n",
        "    def __init__(self, in_channels, channels, patch_size, hidden_channels=32):\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_channels, hidden_channels, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(hidden_channels, channels, patch_size, stride=patch_size)\n",
        "        )"
      ],
      "metadata": {
        "id": "wPYOlFpI7Rzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddPositionEmbedding(nn.Module):\n",
        "    def __init__(self, channels, shape):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.Tensor(channels, *shape))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.pos_embedding"
      ],
      "metadata": {
        "id": "K0OgxaKV7Sb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToEmbedding(nn.Sequential):\n",
        "    def __init__(self, in_channels, channels, patch_size, shape, p_drop=0.):\n",
        "        super().__init__(\n",
        "            ToPatches(in_channels, channels, patch_size),\n",
        "            AddPositionEmbedding(channels, shape),\n",
        "            nn.Dropout(p_drop)\n",
        "        )"
      ],
      "metadata": {
        "id": "IS1Y0kCS7UOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Sequential):\n",
        "    def __init__(self, in_channels, classes, p_drop=0.):\n",
        "        super().__init__(\n",
        "            LayerNormChannels(in_channels),\n",
        "            nn.GELU(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(p_drop),\n",
        "            nn.Linear(in_channels, classes)\n",
        "        )"
      ],
      "metadata": {
        "id": "JMyf6P5K7V1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Sequential):\n",
        "    def __init__(self, classes, image_size, channels, head_channels, num_blocks, patch_size,\n",
        "                 in_channels=3, emb_p_drop=0., trans_p_drop=0., head_p_drop=0.):\n",
        "        reduced_size = image_size // patch_size\n",
        "        shape = (reduced_size, reduced_size)\n",
        "        super().__init__(\n",
        "            ToEmbedding(in_channels, channels, patch_size, shape, emb_p_drop),\n",
        "            TransformerStack(num_blocks, channels, head_channels, shape, trans_p_drop),\n",
        "            Head(channels, classes, head_p_drop)\n",
        "        )\n",
        "        self.reset_parameters()\n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.constant_(m.weight, 1.)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, AddPositionEmbedding):\n",
        "                nn.init.normal_(m.pos_embedding, mean=0.0, std=0.02)\n",
        "            elif isinstance(m, SelfAttention2d):\n",
        "                nn.init.normal_(m.pos_enc, mean=0.0, std=0.02)\n",
        "            elif isinstance(m, Residual):\n",
        "                nn.init.zeros_(m.gamma)\n",
        "    \n",
        "    def separate_parameters(self):\n",
        "        parameters_decay = set()\n",
        "        parameters_no_decay = set()\n",
        "        modules_weight_decay = (nn.Linear, nn.Conv2d)\n",
        "        modules_no_weight_decay = (nn.LayerNorm,)\n",
        "\n",
        "        for m_name, m in self.named_modules():\n",
        "            for param_name, param in m.named_parameters():\n",
        "                full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name\n",
        "\n",
        "                if isinstance(m, modules_no_weight_decay):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif param_name.endswith(\"bias\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, Residual) and param_name.endswith(\"gamma\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, AddPositionEmbedding) and param_name.endswith(\"pos_embedding\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, SelfAttention2d) and param_name.endswith(\"pos_enc\"):\n",
        "                    parameters_no_decay.add(full_param_name)\n",
        "                elif isinstance(m, modules_weight_decay):\n",
        "                    parameters_decay.add(full_param_name)\n",
        "\n",
        "        # sanity check\n",
        "        # assert len(parameters_decay & parameters_no_decay) == 0\n",
        "        # assert len(parameters_decay) + len(parameters_no_decay) == len(list(model.parameters()))\n",
        "\n",
        "        return parameters_decay, parameters_no_decay"
      ],
      "metadata": {
        "id": "Nerb5a6b7bIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES, IMAGE_SIZE = 10, 32\n",
        "model = ViT(NUM_CLASSES, IMAGE_SIZE, channels=32, head_channels=8, num_blocks=4, patch_size=2,\n",
        "               emb_p_drop=0., trans_p_drop=0., head_p_drop=0.1)"
      ],
      "metadata": {
        "id": "DCbQKfVn7crL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)"
      ],
      "metadata": {
        "id": "kLBniG2e7ejz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))"
      ],
      "metadata": {
        "id": "ITG0pdF77hbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model summary"
      ],
      "metadata": {
        "id": "0uKxMPfRE-3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "id": "bNweX8JlFANp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 32\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 2 #25\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-1\n",
        "\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32, scale=(0.75, 1.0), ratio=(1.0, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandAugment(num_ops=1, magnitude=8),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "id": "TfmSvhmw7jp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "clip_norm = True\n",
        "lr_schedule = lambda t: np.interp([t], [0, EPOCHS*2//5, EPOCHS*4//5, EPOCHS], \n",
        "                                  [0, 0.01, 0.01/20.0, 0])[0]\n",
        "\n",
        "model = nn.DataParallel(model, device_ids=[0]).cuda()\n",
        "opt = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss, train_acc, n = 0, 0, 0\n",
        "    for i, (X, y) in enumerate(trainloader):\n",
        "        model.train()\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "\n",
        "        lr = lr_schedule(epoch + (i + 1)/len(trainloader))\n",
        "        opt.param_groups[0].update(lr=lr)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip_norm:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        train_acc += (output.max(1)[1] == y).sum().item()\n",
        "        n += y.size(0)\n",
        "        \n",
        "    model.eval()\n",
        "    test_acc, m = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(testloader):\n",
        "            X, y = X.cuda(), y.cuda()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(X)\n",
        "            test_acc += (output.max(1)[1] == y).sum().item()\n",
        "            m += y.size(0)\n",
        "\n",
        "    print(f'ConvMixer: Epoch: {epoch} | Train Acc: {train_acc/n:.4f}, Test Acc: {test_acc/m:.4f}, Time: {time.time() - start:.1f}, lr: {lr:.6f}')\n"
      ],
      "metadata": {
        "id": "vNqeKdnO7m30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gradcam on 10 Misclassified images"
      ],
      "metadata": {
        "id": "ZZPY46By_fXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install grad-cam"
      ],
      "metadata": {
        "id": "g_EgVJUA_2cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "cY8W0VVjGGCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_layers_name = [name for name, module in model.named_modules() if isinstance(module, nn.GELU)]"
      ],
      "metadata": {
        "id": "iYrpI3niF7qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (target_layers_name)"
      ],
      "metadata": {
        "id": "1QWkfr_UGxjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrong_predictions(model, test_loader, device):\n",
        "    wrong_images=[]\n",
        "    wrong_label=[]\n",
        "    correct_label=[]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)        \n",
        "            pred = output.argmax(dim=1, keepdim=True).squeeze()  # get the index of the max log-probability\n",
        "\n",
        "            wrong_pred = (pred.eq(target.view_as(pred)) == False)\n",
        "            wrong_images.append(data[wrong_pred])\n",
        "            wrong_label.append(pred[wrong_pred])\n",
        "            correct_label.append(target.view_as(pred)[wrong_pred])\n",
        "            wrong_predictions = list(zip(torch.cat(wrong_images),torch.cat(wrong_label),torch.cat(correct_label)))\n",
        "        print(f'Total wrong predictions are {len(wrong_predictions)}')\n",
        "\n",
        "    return wrong_predictions"
      ],
      "metadata": {
        "id": "3qNDflLM_izH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show misclassified images\n",
        "def wrong_plot(n_figures,true,ima,pred, mean, std):\n",
        "    #n_row = int(n_figures/3)\n",
        "    fig = plt.figure(figsize = (10, 5))\n",
        "    fig.tight_layout()\n",
        "    for r in range(n_figures):\n",
        "      a = random.randint(0,len(true)-1)  \n",
        "      img,correct,wrong = ima[a],true[a],pred[a]\n",
        "\n",
        "      f = 'Actual:'+ (classes[correct[0]]) + ',\\n ' +'Predicted:'+(classes[wrong[0]])\n",
        "      #plt.subplot(2,5,r+1)    # the number of images in the grid is 5*5 (25)\n",
        "    \n",
        "      fig.add_subplot(2, 5, r+1)\n",
        "      fig.subplots_adjust(hspace=.25)\n",
        "\n",
        "  \n",
        "      for i in range(img.shape[0]):\n",
        "        img[i] = (img[i]*std[i])+mean[i]\n",
        "    \n",
        "      im = plt.imshow(np.transpose(img, (1,2,0)))\n",
        "     \n",
        "      plt.title(f)\n",
        "    plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "jDyL56Wl_oax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "import cv2\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GradCAM:\n",
        "    \"\"\" Class for extracting activations and \n",
        "    registering gradients from targetted intermediate layers \n",
        "    target_layers = list of convolution layer index as shown in summary\n",
        "    \"\"\"\n",
        "    def __init__(self, model, candidate_layers=None):\n",
        "        def save_fmaps(key):\n",
        "            def forward_hook(module, input, output):\n",
        "                self.fmap_pool[key] = output.detach()\n",
        "\n",
        "            return forward_hook\n",
        "\n",
        "        def save_grads(key):\n",
        "            def backward_hook(module, grad_in, grad_out):\n",
        "                self.grad_pool[key] = grad_out[0].detach()\n",
        "\n",
        "            return backward_hook\n",
        "\n",
        "        self.device = next(model.parameters()).device\n",
        "        self.model = model\n",
        "        self.handlers = []  # a set of hook function handlers\n",
        "        self.fmap_pool = {}\n",
        "        self.grad_pool = {}\n",
        "        self.candidate_layers = candidate_layers  # list\n",
        "\n",
        "        for name, module in self.model.named_modules():\n",
        "            if self.candidate_layers is None or name in self.candidate_layers:\n",
        "                self.handlers.append(module.register_forward_hook(save_fmaps(name)))\n",
        "                self.handlers.append(module.register_backward_hook(save_grads(name)))\n",
        "\n",
        "    def _encode_one_hot(self, ids):\n",
        "        one_hot = torch.zeros_like(self.nll).to(self.device)\n",
        "        #print(one_hot.shape)\n",
        "        one_hot.scatter_(1, ids, 1.0)\n",
        "        return one_hot\n",
        "\n",
        "    def forward(self, image):\n",
        "        self.image_shape = image.shape[2:] # HxW\n",
        "        self.nll = self.model(image)\n",
        "        #self.probs = F.softmax(self.logits, dim=1)\n",
        "        return self.nll.sort(dim=1, descending=True)  # ordered results\n",
        "\n",
        "    def backward(self, ids):\n",
        "        \"\"\"\n",
        "        Class-specific backpropagation\n",
        "        \"\"\"\n",
        "        one_hot = self._encode_one_hot(ids)\n",
        "        self.model.zero_grad()\n",
        "        self.nll.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "    def remove_hook(self):\n",
        "        \"\"\"\n",
        "        Remove all the forward/backward hook functions\n",
        "        \"\"\"\n",
        "        for handle in self.handlers:\n",
        "            handle.remove()\n",
        "\n",
        "    def _find(self, pool, target_layer):\n",
        "        if target_layer in pool.keys():\n",
        "            return pool[target_layer]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid layer name: {}\".format(target_layer))\n",
        "\n",
        "    def generate(self, target_layer):\n",
        "        fmaps = self._find(self.fmap_pool, target_layer)\n",
        "        grads = self._find(self.grad_pool, target_layer)\n",
        "        weights = F.adaptive_avg_pool2d(grads, 1)\n",
        "\n",
        "        gcam = torch.mul(fmaps, weights).sum(dim=1, keepdim=True)\n",
        "        gcam = F.relu(gcam)\n",
        "        # need to capture image size duign forward pass\n",
        "        gcam = F.interpolate(\n",
        "            gcam, self.image_shape, mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "\n",
        "        # scale output between 0,1\n",
        "        B, C, H, W = gcam.shape\n",
        "        gcam = gcam.view(B, -1)\n",
        "        gcam -= gcam.min(dim=1, keepdim=True)[0]\n",
        "        gcam /= gcam.max(dim=1, keepdim=True)[0]\n",
        "        gcam = gcam.view(B, C, H, W)\n",
        "\n",
        "        return gcam\n",
        "\n",
        "def generate_gradcam(misclassified_images, model, target_layers, device):\n",
        "    images=[]\n",
        "    labels=[]\n",
        "    for i, (img, pred, correct) in enumerate(misclassified_images):\n",
        "        images.append(img)\n",
        "        labels.append(correct)\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # map input to device\n",
        "    images = torch.stack(images).to(device)\n",
        "    \n",
        "    # set up grad cam\n",
        "    gcam = GradCAM(model, target_layers)\n",
        "    \n",
        "    # forward pass\n",
        "    probs, ids = gcam.forward(images)\n",
        "    \n",
        "    # outputs agaist which to compute gradients\n",
        "    ids_ = torch.LongTensor(labels).view(len(images),-1).to(device)\n",
        "    \n",
        "    # backward pass\n",
        "    gcam.backward(ids=ids_)\n",
        "    layers = []\n",
        "    for i in range(len(target_layers)):\n",
        "        target_layer = target_layers[i]\n",
        "        print(\"Generating Grad-CAM @{}\".format(target_layer))\n",
        "        # Grad-CAM\n",
        "        layers.append(gcam.generate(target_layer=target_layer))\n",
        "        \n",
        "    # remove hooks when done\n",
        "    gcam.remove_hook()\n",
        "    return layers, probs, ids\n",
        "\n",
        "def plot_gradcam(gcam_layers, target_layers, class_names, image_size, predicted, misclassified_images, mean, std):\n",
        "    \n",
        "    images=[]\n",
        "    labels=[]\n",
        "    for i, (img, pred, correct) in enumerate(misclassified_images):\n",
        "        images.append(img)\n",
        "        labels.append(correct)\n",
        "\n",
        "    c = len(images)+1\n",
        "    r = len(target_layers)+2\n",
        "    fig = plt.figure(figsize=(30,14))\n",
        "    fig.subplots_adjust(hspace=0.01, wspace=0.01)\n",
        "    ax = plt.subplot(r, c, 1)\n",
        "    ax.text(0.3,-0.5, \"INPUT\", fontsize=14)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    for i in range(len(target_layers)):\n",
        "        target_layer = target_layers[i]\n",
        "        ax = plt.subplot(r, c, c*(i+1)+1)\n",
        "        ax.text(0.3,-0.5, target_layer, fontsize=14)\n",
        "        plt.axis('off')\n",
        "\n",
        "        for j in range(len(images)):\n",
        "            #print(\"processing image \" + str(j+1))\n",
        "            img = np.uint8(255*unnormalize(images[j].view(image_size), mean, std))\n",
        "            if i==0:\n",
        "                ax = plt.subplot(r, c, j+2)\n",
        "                ax.text(0, 0.2, f\"actual: {class_names[labels[j]]} \\npredicted: {class_names[predicted[j][0]]}\", fontsize=12)\n",
        "                plt.axis('off')\n",
        "                plt.subplot(r, c, c+j+2)\n",
        "                plt.imshow(img)\n",
        "                plt.axis('off')\n",
        "                \n",
        "                heatmap = 1-gcam_layers[i][j].cpu().numpy()[0] # reverse the color map\n",
        "                heatmap = np.uint8(255 * heatmap)\n",
        "                heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "                superimposed_img = cv2.resize(cv2.addWeighted(img, 0.5, heatmap, 0.5, 0), (128,128))\n",
        "                plt.subplot(r, c, (i+2)*c+j+2)\n",
        "                plt.imshow(superimposed_img, interpolation='bilinear')\n",
        "                plt.axis('off')\n",
        "        \n",
        "    plt.show()\n",
        "    \n",
        "def unnormalize(img, mean, std):\n",
        "    img = img.cpu().numpy().astype(dtype=np.float32)\n",
        "  \n",
        "    for i in range(img.shape[0]):\n",
        "        img[i] = (img[i]*std[i])+mean[i]\n",
        "  \n",
        "    return np.transpose(img, (1,2,0))"
      ],
      "metadata": {
        "id": "Epjm51nwACpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2471, 0.2435, 0.2616)\n",
        "target_layers = [\"module.2.1\"]\n",
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "misclassified_images  = wrong_predictions(model, testloader, DEVICE)\n",
        "gradcam_output, probs, predicted_classes = generate_gradcam(misclassified_images[:10], model, target_layers, DEVICE)\n",
        "plot_gradcam(gradcam_output, target_layers, classes, (3, 32, 32),predicted_classes, misclassified_images[:10], mean, std)"
      ],
      "metadata": {
        "id": "70r3JKbFAe_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}